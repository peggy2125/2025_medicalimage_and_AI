{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83274d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–æ•°æ®\n",
    "train_csv_path = r\"C:\\Users\\Vivo\\2025_medicalimage_and_AI\\MURA-v1.1\\train_labeled_studies.csv\"\n",
    "valid_csv_path = r\"C:\\Users\\Vivo\\2025_medicalimage_and_AI\\MURA-v1.1\\valid_labeled_studies.csv\"\n",
    "data_root = r\"C:\\Users\\Vivo\\2025_medicalimage_and_AI\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76bf6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1)\n",
    "])\n",
    "\n",
    "# è·å–ç—…äººç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_patient_stats(csv_path, mode):\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "    stats = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        study_rel_path, label = row['path'], row['label']\n",
    "        study_path = os.path.join(data_root, study_rel_path)\n",
    "        image_paths = glob(os.path.join(study_path, \"*.png\"))\n",
    "\n",
    "        pixel_means = []\n",
    "        pixel_stds = []\n",
    "\n",
    "        for img_path in image_paths:\n",
    "            img = Image.open(img_path).convert(\"L\")\n",
    "            img = transform(img)  # ä½ å¯ä»¥ç”¨é€‚å½“çš„ transform\n",
    "            img_np = np.array(img).astype(np.float32) / 255.0\n",
    "            pixel_means.append(img_np.mean())\n",
    "            pixel_stds.append(img_np.std())\n",
    "\n",
    "        patient_mean = np.mean(pixel_means)\n",
    "        patient_std = np.mean(pixel_stds)\n",
    "        body_part = study_rel_path.split(\"/\")[2]\n",
    "\n",
    "        stats.append({\n",
    "            \"path\": study_rel_path,  # åŠ å…¥ path\n",
    "            \"body_part\": body_part,\n",
    "            \"label\": label,\n",
    "            \"mean\": patient_mean,\n",
    "            \"std\": patient_std,\n",
    "            \"mode\": mode\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582b9c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13457 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13457/13457 [05:43<00:00, 39.19it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1199/1199 [00:11<00:00, 104.87it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç—…äººç»Ÿè®¡ä¿¡æ¯\n",
    "train_stats = get_patient_stats(train_csv_path, \"train\")\n",
    "valid_stats = get_patient_stats(valid_csv_path, \"valid\")\n",
    "\n",
    "# åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "#df = pd.concat([train_stats, valid_stats], ignore_index=True)\n",
    "#print(df.head())\n",
    "#print(df.shape)  # æ£€æŸ¥æ•°æ®é›†å¤§å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e18ca2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–°çš„è¨“ç·´é›†åˆ†å¸ƒï¼š\n",
      "label\n",
      "0    7479\n",
      "1    4252\n",
      "Name: count, dtype: int64\n",
      "\n",
      "éªŒè¯é›†åˆ†å¸ƒï¼š\n",
      "label\n",
      "1    1463\n",
      "0    1462\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Household é©—è­‰é›†åˆ†å¸ƒï¼š\n",
      "label\n",
      "0    943\n",
      "1    524\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# æ­¥éª¤ 1ï¼šåˆ†å±‚æŠ½æ ·ä¸ KMeans + PCA èšç±»\n",
    "stratified = df.groupby('body_part')\n",
    "\n",
    "df_train_new = []\n",
    "df_valid_new = []\n",
    "\n",
    "# å…ˆä¾ body_part åˆ†å±¤\n",
    "for body_part, body_group in df.groupby('body_part'):\n",
    "    label_counts = body_group['label'].value_counts(normalize=True)\n",
    "    label_ratio_0 = label_counts.get(0, 0)\n",
    "    label_ratio_1 = label_counts.get(1, 0)\n",
    "\n",
    "    # æ ¹æ®åŸå§‹æ¯”ä¾‹è®¡ç®—éªŒè¯é›†çš„ç›®æ ‡å¤§å°\n",
    "    total_samples = len(body_group)\n",
    "    valid_size = int(total_samples * 0.2)  # éªŒè¯é›†å¤§å°ä¸º 20%\n",
    "\n",
    "    # è®¡ç®—éªŒè¯é›†æ ‡ç­¾ 0 å’Œæ ‡ç­¾ 1 çš„ç›®æ ‡æ ·æœ¬æ•°\n",
    "    valid_size_0 = int(valid_size * (label_ratio_0))\n",
    "    valid_size_1 = valid_size - valid_size_0  # å‰©ä½™çš„éªŒè¯é›†å¤§å°ä½œä¸ºæ ‡ç­¾ 1 çš„æ ·æœ¬æ•°\n",
    "\n",
    "    # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„æ–°å€ç‡\n",
    "    new_ratio_0 = (valid_size / 2) / valid_size_0  # è°ƒæ•´åå€ç‡ï¼Œç¡®ä¿æ ‡ç­¾ 0 å’Œæ ‡ç­¾ 1 æ•°é‡å¹³è¡¡\n",
    "    new_ratio_1 = (valid_size / 2) / valid_size_1  # è°ƒæ•´åå€ç‡ï¼Œç¡®ä¿æ ‡ç­¾ 0 å’Œæ ‡ç­¾ 1 æ•°é‡å¹³è¡¡\n",
    "\n",
    "    # å¼•å…¥éšæœºæ€§ï¼ŒèŒƒå›´ Â±0.03\n",
    "    random_factor = np.random.uniform(-0.03, 0.03)\n",
    "\n",
    "    # è°ƒæ•´å€ç‡ï¼Œç¡®ä¿ä¸¤è€…æ€»å’Œä¸å˜\n",
    "    adjusted_ratio_0 = new_ratio_0 + random_factor\n",
    "    adjusted_ratio_1 = new_ratio_1 - random_factor\n",
    "\n",
    "    # è®¡ç®—å®é™…æŠ½æ ·æ•°é‡\n",
    "    label_group_0 = body_group[body_group['label'] == 0]\n",
    "    label_group_1 = body_group[body_group['label'] == 1]\n",
    "\n",
    "    # æ ¹æ®è°ƒæ•´åçš„å€ç‡è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„æŠ½æ ·æ•°é‡\n",
    "    label_group_valid_0 = label_group_0.sample(n=int(valid_size_0 * adjusted_ratio_0), random_state=42)\n",
    "    label_group_valid_1 = label_group_1.sample(n=int(valid_size_1 * adjusted_ratio_1), random_state=42)\n",
    "\n",
    "    # å‰©ä½™æ ·æœ¬ä½œä¸ºè®­ç»ƒé›†\n",
    "    label_group_train_0 = label_group_0.drop(label_group_valid_0.index)\n",
    "    label_group_train_1 = label_group_1.drop(label_group_valid_1.index)\n",
    "\n",
    "    # åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    df_train_new.append(label_group_train_0)\n",
    "    df_train_new.append(label_group_train_1)\n",
    "    df_valid_new.append(label_group_valid_0)\n",
    "    df_valid_new.append(label_group_valid_1)\n",
    "\n",
    "# åˆå¹¶æœ€ç»ˆçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "df_train_final = pd.concat(df_train_new, ignore_index=True)\n",
    "df_valid_final = pd.concat(df_valid_new, ignore_index=True)\n",
    "\n",
    "# ç¡®ä¿æ¯è¡Œåªæœ‰è·¯å¾„å’Œæ ‡ç­¾\n",
    "df_train_final_c = df_train_final[['path', 'label']]\n",
    "df_valid_final_c = df_valid_final[['path', 'label']]\n",
    "\n",
    "df_valid_final_c.to_csv('valid_cluster_split.csv', index=False, header=False)\n",
    "\n",
    "'''åŠ ä¸Šhousehold'''\n",
    "# åŠ å…¥ body_part è³‡è¨Šï¼ˆå› ç‚ºä¹‹å‰è¼¸å‡ºçš„æ˜¯ path å’Œ labelï¼Œæ‰€ä»¥å…ˆå¾åŸå§‹ df ä¸­è£œä¸Šï¼‰\n",
    "df_train_full = pd.merge(df_train_final_c, df[['path', 'body_part']], on='path', how='left')\n",
    "\n",
    "# åˆå§‹åŒ–æ–°çš„ train å’Œ household list\n",
    "df_train_reduced = []\n",
    "df_household_valid = []\n",
    "\n",
    "# ä¾ body_part åˆ†å±¤æŠ½å‡º 1/8 ä½œç‚º household valid\n",
    "for body_part, group in df_train_full.groupby('body_part'):\n",
    "    household_sample = group.sample(frac=1/8, random_state=42)\n",
    "    remaining = group.drop(household_sample.index)\n",
    "\n",
    "    df_household_valid.append(household_sample)\n",
    "    df_train_reduced.append(remaining)\n",
    "\n",
    "# åˆä½µè³‡æ–™\n",
    "df_train_final_reduce = pd.concat(df_train_reduced, ignore_index=True)\n",
    "df_train_final_c_reduce = df_train_final_reduce[['path', 'label']]\n",
    "df_household_final = pd.concat(df_household_valid, ignore_index=True)\n",
    "df_household_final_c = df_household_final[['path', 'label']]\n",
    "\n",
    "# è¼¸å‡ºç‚º CSV\n",
    "df_train_final_c.to_csv('train_cluster_split.csv', index=False, header=False)\n",
    "df_household_final_c.to_csv('household_valid.csv', index=False, header=False)\n",
    "\n",
    "# é¡¯ç¤ºåˆ†ä½ˆ\n",
    "print(\"æ–°çš„è¨“ç·´é›†åˆ†å¸ƒï¼š\")\n",
    "print(df_train_final_c['label'].value_counts())\n",
    "\n",
    "print(\"\\néªŒè¯é›†åˆ†å¸ƒï¼š\")\n",
    "print(df_valid_final_c['label'].value_counts())\n",
    "\n",
    "print(\"\\nHousehold é©—è­‰é›†åˆ†å¸ƒï¼š\")\n",
    "print(df_household_final_c['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b75815bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Train Data Distribution:\n",
      "label           0     1\n",
      "body_part              \n",
      "XR_ELBOW      881   459\n",
      "XR_FINGER    1009   469\n",
      "XR_FOREARM    491   218\n",
      "XR_HAND      1196   331\n",
      "XR_HUMERUS    275   235\n",
      "XR_SHOULDER  1020  1091\n",
      "XR_WRIST     1664   925\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š Valid Data Distribution:\n",
      "label          0    1\n",
      "body_part            \n",
      "XR_ELBOW     187  193\n",
      "XR_FINGER    211  210\n",
      "XR_FOREARM    98  102\n",
      "XR_HAND      225  215\n",
      "XR_HUMERUS    71   73\n",
      "XR_SHOULDER  301  301\n",
      "XR_WRIST     369  369\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š Household Valid Data Distribution:\n",
      "label          0    1\n",
      "body_part            \n",
      "XR_ELBOW     118   74\n",
      "XR_FINGER    152   59\n",
      "XR_FOREARM    70   31\n",
      "XR_HAND      177   41\n",
      "XR_HUMERUS    43   30\n",
      "XR_SHOULDER  142  160\n",
      "XR_WRIST     241  129\n",
      "----------------------------------------\n",
      "shape of train: (10264, 3)\n",
      "shape of valid: (2925, 6)\n",
      "shape of household valid: (1467, 3)\n",
      "âœ… Done! New train/valid csv saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# æ‰“å°åˆ†å¸ƒæŠ¥å‘Š\n",
    "def show_distribution(df_new, name):\n",
    "    dist = df_new.groupby([\"body_part\", \"label\"]).size().unstack(fill_value=0)\n",
    "    print(f\"\\nğŸ“Š {name} Data Distribution:\")\n",
    "    print(dist)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "show_distribution(df_train_final_reduce, \"Train\")\n",
    "show_distribution(df_valid_final, \"Valid\")\n",
    "show_distribution(df_household_final, \"Household Valid\")\n",
    "# è¾“å‡ºæœ€ç»ˆçš„ train å’Œ valid æ•°æ®é›†\n",
    "\n",
    "print(\"shape of train:\", df_train_final_reduce.shape)\n",
    "print(\"shape of valid:\", df_valid_final.shape)\n",
    "print(\"shape of household valid:\", df_household_final.shape)\n",
    "print(\"âœ… Done! New train/valid csv saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2570f1ca",
   "metadata": {},
   "source": [
    "## ä¸ä½¿ç”¨kmeans_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd33bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–°çš„è¨“ç·´é›†åˆ†å¸ƒï¼š\n",
      "label\n",
      "0    6535\n",
      "1    3734\n",
      "Name: count, dtype: int64\n",
      "\n",
      "éªŒè¯é›†åˆ†å¸ƒï¼ˆå·²å¹³è¡¡ï¼‰ï¼š\n",
      "label\n",
      "0    1483\n",
      "1    1441\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Household é©—è­‰é›†åˆ†å¸ƒï¼ˆç„¡éœ€å¹³è¡¡ï¼‰ï¼š\n",
      "label\n",
      "0    923\n",
      "1    540\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_train_new = []\n",
    "df_valid_new = []\n",
    "\n",
    "# åˆ† stratified validï¼šæ¯å€‹ body_part å„è‡ªè™•ç†\n",
    "for body_part, body_group in df.groupby('body_part'):\n",
    "    # å„é¡åˆ¥åˆ†é–‹\n",
    "    label_0 = body_group[body_group['label'] == 0]\n",
    "    label_1 = body_group[body_group['label'] == 1]\n",
    "\n",
    "    # è¨ˆç®—æ¯é¡åˆ¥è©²æŠ½å¤šå°‘æ¨£æœ¬ï¼ˆæ­£è² é¡å„ 10%ï¼Œç¸½å…± valid ä½” 20%ï¼‰\n",
    "    # éš¨æ©Ÿæ€§ç¯„åœ Â±0.03\n",
    "    random_factor = np.random.uniform(-0.03, 0.03)\n",
    "    valid_size_class_0 = int(len(body_group) *(0.1 + random_factor))\n",
    "    valid_size_class_1 = int(len(body_group) *(0.1 - random_factor))\n",
    "\n",
    "    # è‹¥æ•¸é‡ä¸è¶³ï¼Œä½¿ç”¨ min é˜²æ­¢éŒ¯èª¤\n",
    "    sample_0 = label_0.sample(n=min(valid_size_class_0, len(label_0)), random_state=42)\n",
    "    sample_1 = label_1.sample(n=min(valid_size_class_1, len(label_1)), random_state=42)\n",
    "\n",
    "    # åˆä½µæˆ valid\n",
    "    valid_part = pd.concat([sample_0, sample_1])\n",
    "    train_part = body_group.drop(valid_part.index)\n",
    "\n",
    "    df_valid_new.append(valid_part)\n",
    "    df_train_new.append(train_part)\n",
    "\n",
    "# åˆä½µæ‰€æœ‰ body_part çš„ train å’Œ valid\n",
    "df_train_final = pd.concat(df_train_new, ignore_index=True)\n",
    "df_valid_final = pd.concat(df_valid_new, ignore_index=True)\n",
    "\n",
    "# ä¿ç•™ path å’Œ label æ¬„\n",
    "df_train_final_c = df_train_final[['path', 'label']]\n",
    "df_valid_final_c = df_valid_final[['path', 'label']]\n",
    "df_valid_final_c.to_csv('valid_balanced_random.csv', index=False, header=False)\n",
    "\n",
    "# householdï¼šå¾ train_final ä¸­å–æ¯å€‹ body_part çš„ 10%ï¼ˆç„¡éœ€å¹³è¡¡ï¼‰\n",
    "df_train_full = pd.merge(df_train_final_c, df[['path', 'body_part']], on='path', how='left')\n",
    "\n",
    "df_train_reduced = []\n",
    "df_household_valid = []\n",
    "\n",
    "for body_part, group in df_train_full.groupby('body_part'):\n",
    "    n_household = int(len(group) / 8)  # æ˜ç¢ºè¡¨ç¤ºæ˜¯å–1/8\n",
    "    household_sample = group.sample(n=n_household, random_state=42)\n",
    "    remaining = group.drop(household_sample.index)\n",
    "\n",
    "    df_household_valid.append(household_sample)\n",
    "    df_train_reduced.append(remaining)\n",
    "\n",
    "# åˆä½µ\n",
    "df_train_final_reduce = pd.concat(df_train_reduced, ignore_index=True)\n",
    "df_train_final_c_reduce = df_train_final_reduce[['path', 'label']]\n",
    "df_household_final = pd.concat(df_household_valid, ignore_index=True)\n",
    "df_household_final_c = df_household_final[['path', 'label']]\n",
    "\n",
    "# è¼¸å‡º CSV\n",
    "df_train_final_c_reduce.to_csv('train_random_split.csv', index=False, header=False)\n",
    "df_household_final_c.to_csv('household_random.csv', index=False, header=False)\n",
    "\n",
    "# é¡¯ç¤ºåˆ†å¸ƒ\n",
    "print(\"æ–°çš„è¨“ç·´é›†åˆ†å¸ƒï¼š\")\n",
    "print(df_train_final_c_reduce['label'].value_counts())\n",
    "\n",
    "print(\"\\néªŒè¯é›†åˆ†å¸ƒï¼ˆå·²å¹³è¡¡ï¼‰ï¼š\")\n",
    "print(df_valid_final_c['label'].value_counts())\n",
    "\n",
    "print(\"\\nHousehold é©—è­‰é›†åˆ†å¸ƒï¼ˆç„¡éœ€å¹³è¡¡ï¼‰ï¼š\")\n",
    "print(df_household_final_c['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e19960c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Train Data Distribution:\n",
      "label           0     1\n",
      "body_part              \n",
      "XR_ELBOW      859   481\n",
      "XR_FINGER    1026   452\n",
      "XR_FOREARM    499   209\n",
      "XR_HAND      1185   346\n",
      "XR_HUMERUS    280   231\n",
      "XR_SHOULDER  1078  1034\n",
      "XR_WRIST     1608   981\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š Valid Data Distribution:\n",
      "label          0    1\n",
      "body_part            \n",
      "XR_ELBOW     200  181\n",
      "XR_FINGER    203  218\n",
      "XR_FOREARM    82  119\n",
      "XR_HAND      243  193\n",
      "XR_HUMERUS    73   71\n",
      "XR_SHOULDER  230  372\n",
      "XR_WRIST     452  287\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“Š Household Valid Data Distribution:\n",
      "label          0    1\n",
      "body_part            \n",
      "XR_ELBOW     127   64\n",
      "XR_FINGER    143   68\n",
      "XR_FOREARM    78   23\n",
      "XR_HAND      170   48\n",
      "XR_HUMERUS    36   36\n",
      "XR_SHOULDER  155  146\n",
      "XR_WRIST     214  155\n",
      "----------------------------------------\n",
      "shape of train: (10269, 3)\n",
      "shape of valid: (2924, 6)\n",
      "shape of household valid: (1463, 3)\n",
      "âœ… Done! New train/valid csv saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# æ‰“å°åˆ†å¸ƒæŠ¥å‘Š\n",
    "def show_distribution(df_new, name):\n",
    "    dist = df_new.groupby([\"body_part\", \"label\"]).size().unstack(fill_value=0)\n",
    "    print(f\"\\nğŸ“Š {name} Data Distribution:\")\n",
    "    print(dist)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "show_distribution(df_train_final_reduce, \"Train\")\n",
    "show_distribution(df_valid_final, \"Valid\")\n",
    "show_distribution(df_household_final, \"Household Valid\")\n",
    "# è¾“å‡ºæœ€ç»ˆçš„ train å’Œ valid æ•°æ®é›†\n",
    "\n",
    "print(\"shape of train:\", df_train_final_reduce.shape)\n",
    "print(\"shape of valid:\", df_valid_final.shape)\n",
    "print(\"shape of household valid:\", df_household_final.shape)\n",
    "print(\"âœ… Done! New train/valid csv saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a62fab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_stats shape: label\n",
      "0    8280\n",
      "1    5177\n",
      "Name: count, dtype: int64\n",
      "æ–°çš„è¨“ç·´é›†åˆ†å¸ƒï¼š\n",
      "label\n",
      "0    7463\n",
      "1    4648\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Household é©—è­‰é›†åˆ†å¸ƒï¼ˆç„¡éœ€å¹³è¡¡ï¼‰ï¼š\n",
      "label\n",
      "0    817\n",
      "1    529\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train_reduced = []\n",
    "df_household_valid = []\n",
    "print(\"train_stats shape:\", train_stats['label'].value_counts())\n",
    "for body_part, group in train_stats.groupby('body_part'):\n",
    "    household_sample = group.sample(frac=0.1, random_state=42)\n",
    "\n",
    "    remaining = group.drop(household_sample.index)\n",
    "\n",
    "    df_household_valid.append(household_sample)\n",
    "    df_train_reduced.append(remaining)\n",
    "\n",
    "# åˆä½µ\n",
    "df_train_final_reduce = pd.concat(df_train_reduced, ignore_index=True)\n",
    "df_train_final_c_reduce = df_train_final_reduce[['path', 'label']]\n",
    "df_household_final = pd.concat(df_household_valid, ignore_index=True)\n",
    "df_household_final_c = df_household_final[['path', 'label']]\n",
    "\n",
    "# è¼¸å‡º CSV\n",
    "df_train_final_c_reduce.to_csv('origin_train.csv', index=False, header=False)\n",
    "df_household_final_c.to_csv('household_for_real.csv', index=False, header=False)\n",
    "\n",
    "# é¡¯ç¤ºåˆ†å¸ƒ\n",
    "print(\"æ–°çš„è¨“ç·´é›†åˆ†å¸ƒï¼š\")\n",
    "print(df_train_final_c_reduce['label'].value_counts())\n",
    "\n",
    "print(\"\\nHousehold é©—è­‰é›†åˆ†å¸ƒï¼ˆç„¡éœ€å¹³è¡¡ï¼‰ï¼š\")\n",
    "print(df_household_final_c['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntuml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
