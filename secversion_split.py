import os
import numpy as np
import pandas as pd
from PIL import Image
from glob import glob
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import argparse
import logging
from torchvision import transforms
import random
from pathlib import Path
import seaborn as sns

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("MURA-Splitter")

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='MURAæ•°æ®é›†å¹³è¡¡åˆ†å‰²å·¥å…·')
    parser.add_argument('--data_root', type=str, default='MURA-v1.1', 
                        help='MURAæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--train_csv', type=str, default='MURA-v1.1/train_labeled_studies.csv',
                        help='åŸå§‹è®­ç»ƒé›†CSVè·¯å¾„')
    parser.add_argument('--valid_csv', type=str, default='MURA-v1.1/valid_labeled_studies.csv',
                        help='åŸå§‹éªŒè¯é›†CSVè·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='.',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--valid_ratio', type=float, default=0.2,
                        help='éªŒè¯é›†å æ¯”')
    parser.add_argument('--n_clusters', type=int, default=3,
                        help='æ¯ä¸ªç»„å†…çš„èšç±»æ•°é‡')
    parser.add_argument('--random_seed', type=int, default=42,
                        help='éšæœºç§å­')
    parser.add_argument('--min_valid_samples', type=int, default=2,
                        help='æ¯ä¸ªèšç±»çš„æœ€å°éªŒè¯æ ·æœ¬æ•°')
    parser.add_argument('--img_size', type=int, default=224,
                        help='å›¾åƒé¢„å¤„ç†å°ºå¯¸')
    parser.add_argument('--save_full_info', action='store_true',
                        help='æ˜¯å¦ä¿å­˜å®Œæ•´ç‰¹å¾ä¿¡æ¯ï¼ˆç”¨äºåˆ†æï¼‰')
    parser.add_argument('--patient_level', action='store_true', default=True,
                        help='ä»¥ç—…äººä¸ºå•ä½è¿›è¡Œåˆ†å‰²ï¼Œé¿å…æ•°æ®æ³„æ¼')
    return parser.parse_args()

def setup_environment(args):
    """è®¾ç½®ç¯å¢ƒå’Œéšæœºç§å­"""
    random.seed(args.random_seed)
    np.random.seed(args.random_seed)
    os.makedirs(args.output_dir, exist_ok=True)
    return args

def get_transform(img_size):
    """è·å–å›¾åƒé¢„å¤„ç†å˜æ¢"""
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.Grayscale(num_output_channels=1)
    ])

def extract_features(img_path, transform):
    """æå–å•ä¸ªå›¾åƒçš„ç‰¹å¾"""
    try:
        img = Image.open(img_path).convert("L")
        img = transform(img)
        img_np = np.array(img).astype(np.float32) / 255.0
        
        # åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
        pixel_mean = img_np.mean()
        pixel_std = img_np.std()
        
        # ç›´æ–¹å›¾ç‰¹å¾ (ç®€åŒ–ç‰ˆ)
        hist, _ = np.histogram(img_np, bins=10, range=(0, 1))
        hist = hist / hist.sum()  # å½’ä¸€åŒ–
        
        # çº¹ç†ç‰¹å¾ (ç®€åŒ–ç‰ˆ - æ ‡å‡†å·®çš„æ ‡å‡†å·®ä½œä¸ºç²—ç•¥çš„çº¹ç†æŒ‡æ ‡)
        patches = [img_np[i:i+32, j:j+32] for i in range(0, img_np.shape[0], 32) 
                                           for j in range(0, img_np.shape[1], 32)]
        patch_stds = [p.std() for p in patches if p.size > 0]
        texture = np.std(patch_stds) if patch_stds else 0
        
        return {
            'mean': pixel_mean,
            'std': pixel_std,
            'texture': texture,
            'hist': hist.tolist()
        }
    except Exception as e:
        logger.warning(f"å¤„ç†å›¾åƒ {img_path} æ—¶å‡ºé”™: {e}")
        # è¿”å›é»˜è®¤å€¼
        return {
            'mean': 0.5,
            'std': 0.1,
            'texture': 0.0,
            'hist': [0.1] * 10
        }

def get_patient_stats(csv_path, data_root, transform, mode):
    """è·å–æ‚£è€…ç»Ÿè®¡ä¿¡æ¯"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°CSVæ–‡ä»¶: {csv_path}")
        
        df = pd.read_csv(csv_path, header=None, names=['path', 'label'])
        stats = []
        
        for _, row in tqdm(df.iterrows(), total=len(df), desc=f"å¤„ç†{mode}æ•°æ®"):
            study_rel_path, label = row['path'], row['label']
            study_path = os.path.join(data_root, study_rel_path)
            
            if not os.path.exists(study_path):
                logger.warning(f"æ‰¾ä¸åˆ°è·¯å¾„: {study_path}, è·³è¿‡")
                continue
                
            image_paths = glob(os.path.join(study_path, "*.png"))
            if not image_paths:
                logger.warning(f"ç›®å½•ä¸åŒ…å«PNGå›¾åƒ: {study_path}, è·³è¿‡")
                continue

            # æå–èº«ä½“éƒ¨ä½
            body_part = study_rel_path.split("/")[2]  # å‡è®¾æ ¼å¼æ˜¯ "MURA-v1.1/train/XR_PART/..."
            
            # æ”¶é›†æ‰€æœ‰å›¾åƒçš„ç‰¹å¾
            features_list = []
            for img_path in image_paths:
                features = extract_features(img_path, transform)
                features_list.append(features)
            
            # è®¡ç®—ç ”ç©¶çº§åˆ«çš„ç‰¹å¾
            if features_list:
                means = [f['mean'] for f in features_list]
                stds = [f['std'] for f in features_list]
                textures = [f['texture'] for f in features_list]
                hist_avg = np.mean([f['hist'] for f in features_list], axis=0).tolist()
                
                patient_mean = np.mean(means)
                patient_std = np.mean(stds)
                patient_texture = np.mean(textures)
                
                # æ›´å¤šç»Ÿè®¡ä¿¡æ¯
                img_count = len(image_paths)
                
                stats.append({
                    "path": study_rel_path,
                    "body_part": body_part,
                    "label": label,
                    "mean": patient_mean,
                    "std": patient_std,
                    "texture": patient_texture,
                    "hist": hist_avg,
                    "img_count": img_count,
                    "mode": mode
                })
        
        return pd.DataFrame(stats)
    except Exception as e:
        logger.error(f"å¤„ç†CSV {csv_path} æ—¶å‡ºé”™: {e}")
        return pd.DataFrame()

def extract_patient_id(path):
    """ä»è·¯å¾„ä¸­æå–ç—…äººID"""
    # å‡è®¾è·¯å¾„æ ¼å¼ä¸º "MURA-v1.1/train/XR_PART/patient_XXXX/study_Y/image.png"
    # æˆ‘ä»¬æå– patient_XXXX ä½œä¸ºç—…äººID
    parts = path.split('/')
    for part in parts:
        if part.startswith('patient'):
            return part
    return None

def stratified_cluster_split(df, args):
    """ä½¿ç”¨åˆ†å±‚èšç±»è¿›è¡Œè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ†å‰²ï¼Œç¡®ä¿åŒä¸€ç—…äººä¸ä¼šè¢«åˆ†åˆ°ä¸åŒé›†åˆ"""
    logger.info("å¼€å§‹åˆ†å±‚èšç±»åˆ†å‰²...")
    
    # ç‰¹å¾å½’ä¸€åŒ–
    scaler = StandardScaler()
    feature_cols = ['mean', 'std', 'texture']
    
    # æ·»åŠ ç—…äººID
    df['patient_id'] = df['path'].apply(extract_patient_id)
    
    df_train_new = []
    df_valid_new = []
    
    # æŒ‰èº«ä½“éƒ¨ä½åˆ†å±‚
    for body_part, body_group in df.groupby('body_part'):
        logger.info(f"å¤„ç†èº«ä½“éƒ¨ä½: {body_part}")
        
        # å†æŒ‰æ ‡ç­¾åˆ†å±‚
        for label, label_group in body_group.groupby('label'):
            # æŒ‰ç—…äººIDè¿›è¡Œèšåˆï¼Œç¡®ä¿åŒä¸€ç—…äººçš„æ‰€æœ‰æ•°æ®ä¿æŒåœ¨ä¸€èµ·
            patient_groups = []
            
            if args.patient_level:
                # è·å–è¯¥ç»„ä¸­æ‰€æœ‰å”¯ä¸€çš„ç—…äººID
                unique_patients = label_group['patient_id'].unique()
                logger.info(f"éƒ¨ä½ {body_part}, æ ‡ç­¾ {label} çš„å”¯ä¸€ç—…äººæ•°: {len(unique_patients)}")
                
                # æŒ‰ç—…äººIDèšåˆæ•°æ®
                for patient_id in unique_patients:
                    patient_data = label_group[label_group['patient_id'] == patient_id]
                    if not patient_data.empty:
                        # å–è¯¥ç—…äººçš„å¹³å‡ç‰¹å¾ä½œä¸ºèšç±»ä¾æ®
                        patient_features = {
                            'patient_id': patient_id,
                            'mean': patient_data['mean'].mean(),
                            'std': patient_data['std'].mean(),
                            'texture': patient_data['texture'].mean(),
                            'hist': np.mean(np.array(patient_data['hist'].tolist()), axis=0).tolist(),
                            'data': patient_data
                        }
                        patient_groups.append(patient_features)
            else:
                # ä¸æŒ‰ç—…äººèšåˆï¼Œç›´æ¥ä½¿ç”¨æ ·æœ¬
                for _, row in label_group.iterrows():
                    patient_groups.append({
                        'patient_id': row['patient_id'],
                        'mean': row['mean'],
                        'std': row['std'],
                        'texture': row['texture'],
                        'hist': row['hist'],
                        'data': label_group[label_group['patient_id'] == row['patient_id']]
                    })
            
            # ç¡®å®šèšç±»æ•°
            if len(patient_groups) < args.n_clusters:
                logger.warning(f"éƒ¨ä½ {body_part}, æ ‡ç­¾ {label} çš„ç—…äººæ•° ({len(patient_groups)}) å°äºèšç±»æ•° ({args.n_clusters})ï¼Œå‡å°‘èšç±»æ•°")
                n_clusters = max(2, len(patient_groups) // 2)  # è‡³å°‘2ä¸ªèšç±»ï¼Œæˆ–è€…ç—…äººæ•°çš„ä¸€åŠ
            else:
                n_clusters = args.n_clusters
                
            # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
            X = np.array([[pg['mean'], pg['std'], pg['texture']] for pg in patient_groups])
            if len(X) == 0:
                logger.warning(f"éƒ¨ä½ {body_part}, æ ‡ç­¾ {label} æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
                continue
                
            X_scaled = scaler.fit_transform(X)
            
            # ä½¿ç”¨ç›´æ–¹å›¾ç‰¹å¾è¡¥å……
            hist_features = np.array([pg['hist'] for pg in patient_groups])
            combined_features = np.hstack([X_scaled, hist_features])
            
            # PCAé™ç»´ï¼ˆå¦‚æœæ ·æœ¬è¶³å¤Ÿï¼‰
            if len(combined_features) > 5:  # è‡³å°‘éœ€è¦å‡ ä¸ªæ ·æœ¬æ‰èƒ½è¿›è¡ŒPCA
                pca = PCA(n_components=min(0.95, 1.0))  # ç¡®ä¿ä¸ä¼šè¶…è¿‡æ ·æœ¬æ•°
                X_pca = pca.fit_transform(combined_features)
            else:
                X_pca = combined_features
            
            # èšç±»
            clusters = []
            if len(X_pca) > n_clusters:
                kmeans = KMeans(
                    n_clusters=n_clusters, 
                    random_state=args.random_seed,
                    n_init=10
                )
                cluster_ids = kmeans.fit_predict(X_pca)
                
                # å°†èšç±»ç»“æœæ·»åŠ åˆ°ç—…äººç»„
                for i, pg in enumerate(patient_groups):
                    pg['cluster'] = cluster_ids[i]
                    
                # æŒ‰èšç±»IDåˆ†ç»„
                for cluster_id in range(n_clusters):
                    clusters.append([pg for pg in patient_groups if pg['cluster'] == cluster_id])
            else:
                # æ ·æœ¬å¤ªå°‘ï¼Œæ¯ä¸ªç—…äººä¸€ä¸ªèšç±»
                for i, pg in enumerate(patient_groups):
                    pg['cluster'] = i
                    clusters.append([pg])
            
            # ä»æ¯ä¸ªèšç±»ä¸­æŠ½å–è®­ç»ƒé›†å’ŒéªŒè¯é›†
            for cluster_idx, cluster in enumerate(clusters):
                if not cluster:  # ç©ºèšç±»
                    continue
                    
                # è®¡ç®—éªŒè¯é›†å¤§å°
                cluster_size = len(cluster)
                valid_size = max(
                    min(args.min_valid_samples, cluster_size // 2),
                    int(cluster_size * args.valid_ratio)
                )
                valid_size = min(valid_size, cluster_size - 1)  # ç¡®ä¿è‡³å°‘æœ‰1ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ
                
                if valid_size <= 0 or cluster_size <= 1:
                    # æ ·æœ¬å¤ªå°‘ï¼Œå…¨éƒ¨ç”¨äºè®­ç»ƒ
                    train_patients = cluster
                    valid_patients = []
                else:
                    # éšæœºæŠ½æ ·
                    random.shuffle(cluster)
                    valid_patients = cluster[:valid_size]
                    train_patients = cluster[valid_size:]
                
                # æ”¶é›†è®­ç»ƒå’ŒéªŒè¯æ•°æ®
                for pg in train_patients:
                    df_train_new.append(pg['data'])
                
                for pg in valid_patients:
                    df_valid_new.append(pg['data'])
                
                # è®°å½•åˆ†é…æƒ…å†µ
                train_samples = sum(len(pg['data']) for pg in train_patients)
                valid_samples = sum(len(pg['data']) for pg in valid_patients)
                logger.info(f"éƒ¨ä½: {body_part}, æ ‡ç­¾: {label}, èšç±»: {cluster_idx}, "
                           f"æ€»ç—…äºº: {cluster_size}, è®­ç»ƒç—…äºº: {len(train_patients)}, éªŒè¯ç—…äºº: {len(valid_patients)}, "
                           f"è®­ç»ƒæ ·æœ¬: {train_samples}, éªŒè¯æ ·æœ¬: {valid_samples}")
    
    # åˆå¹¶ç»“æœ
    df_train_final = pd.concat(df_train_new, ignore_index=True) if df_train_new else pd.DataFrame()
    df_valid_final = pd.concat(df_valid_new, ignore_index=True) if df_valid_new else pd.DataFrame()
    
    # ç¡®è®¤æ²¡æœ‰ç—…äººåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†
    if not df_train_final.empty and not df_valid_final.empty:
        train_patients = set(df_train_final['patient_id'])
        valid_patients = set(df_valid_final['patient_id'])
        overlap = train_patients.intersection(valid_patients)
        
        if overlap:
            logger.warning(f"å‘ç° {len(overlap)} ä¸ªç—…äººåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ï¼")
            for patient in overlap:
                logger.warning(f"é‡å ç—…äºº: {patient}")
        else:
            logger.info("âœ“ éªŒè¯å®Œæˆï¼šæ²¡æœ‰ç—…äººåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­")
    
    return df_train_final, df_valid_final

def save_results(df_train, df_valid, args):
    """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶ï¼ˆåªä¿å­˜pathå’Œlabelï¼‰"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æå–éœ€è¦çš„åˆ—
    df_train_csv = df_train[['path', 'label']]
    df_valid_csv = df_valid[['path', 'label']]
    
    # ä¿å­˜CSV - åªåŒ…å«pathå’Œlabelï¼Œæ— è¡¨å¤´
    train_csv_path = os.path.join(args.output_dir, 'train_balanced_split.csv')
    valid_csv_path = os.path.join(args.output_dir, 'valid_balanced_split.csv')
    
    df_train_csv.to_csv(train_csv_path, index=False, header=False)
    df_valid_csv.to_csv(valid_csv_path, index=False, header=False)
    
    logger.info(f"å·²ä¿å­˜è®­ç»ƒé›†CSV: {train_csv_path}")
    logger.info(f"å·²ä¿å­˜éªŒè¯é›†CSV: {valid_csv_path}")
    
    # å¯é€‰ï¼šä¿å­˜å®Œæ•´ä¿¡æ¯ç”¨äºåˆ†æï¼ˆä¸ç”¨äºè®­ç»ƒï¼‰
    if args.save_full_info:
        train_full_path = os.path.join(args.output_dir, 'train_full_info.csv')
        valid_full_path = os.path.join(args.output_dir, 'valid_full_info.csv')
        
        # è½¬æ¢histåˆ—ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿ä¿å­˜
        df_train['hist_str'] = df_train['hist'].apply(lambda x: ','.join(map(str, x)))
        df_valid['hist_str'] = df_valid['hist'].apply(lambda x: ','.join(map(str, x)))
        
        # åˆ é™¤åŸå§‹histåˆ—
        df_train_full = df_train.drop('hist', axis=1)
        df_valid_full = df_valid.drop('hist', axis=1)
        
        df_train_full.to_csv(train_full_path, index=False)
        df_valid_full.to_csv(valid_full_path, index=False)
        
        logger.info(f"å·²ä¿å­˜è®­ç»ƒé›†å®Œæ•´ä¿¡æ¯: {train_full_path}")
        logger.info(f"å·²ä¿å­˜éªŒè¯é›†å®Œæ•´ä¿¡æ¯: {valid_full_path}")

def visualize_distribution(df_train, df_valid, args):
    """å¯è§†åŒ–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ†å¸ƒ"""
    # è®¾ç½®ç»˜å›¾æ ·å¼
    plt.style.use('seaborn-v0_8-whitegrid')
    
    # å›¾1: ä¸åŒèº«ä½“éƒ¨ä½çš„æ ·æœ¬æ•°é‡åˆ†å¸ƒ
    plt.figure(figsize=(12, 6))
    
    # å‡†å¤‡æ•°æ®
    body_parts = sorted(df_train['body_part'].unique())
    train_count = df_train['body_part'].value_counts().reindex(body_parts).fillna(0)
    valid_count = df_valid['body_part'].value_counts().reindex(body_parts).fillna(0)
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    x = np.arange(len(body_parts))
    width = 0.35
    
    plt.bar(x - width/2, train_count, width, label='è®­ç»ƒé›†')
    plt.bar(x + width/2, valid_count, width, label='éªŒè¯é›†')
    
    plt.ylabel('æ ·æœ¬æ•°é‡')
    plt.title('å„èº«ä½“éƒ¨ä½æ ·æœ¬åˆ†å¸ƒ')
    plt.xticks(x, body_parts, rotation=45)
    plt.legend()
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plt.savefig(os.path.join(args.output_dir, 'body_part_distribution.png'), dpi=300)
    
    # å›¾2: æ¯ä¸ªèº«ä½“éƒ¨ä½çš„æ­£è´Ÿç±»æ¯”ä¾‹
    plt.figure(figsize=(15, 10))
    
    # åˆå¹¶æ•°æ®é›†ä»¥è®¡ç®—æ¯ä¸ªéƒ¨ä½çš„æ­£è´Ÿç±»æ¯”ä¾‹
    df_combined = pd.concat([
        df_train.assign(dataset='è®­ç»ƒé›†'),
        df_valid.assign(dataset='éªŒè¯é›†')
    ])
    
    # ä½¿ç”¨seabornè¿›è¡Œç»˜åˆ¶
    sns.countplot(data=df_combined, x='body_part', hue='label', col='dataset', col_wrap=1)
    plt.title('å„èº«ä½“éƒ¨ä½æ­£è´Ÿç±»åˆ†å¸ƒ')
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plt.savefig(os.path.join(args.output_dir, 'class_distribution.png'), dpi=300)
    
    # å›¾3: ç‰¹å¾åˆ†å¸ƒå¯è§†åŒ–
    plt.figure(figsize=(15, 15))
    
    feature_cols = ['mean', 'std', 'texture']
    
    for i, feature in enumerate(feature_cols):
        plt.subplot(3, 1, i+1)
        sns.kdeplot(data=df_train, x=feature, hue='body_part', common_norm=False)
        plt.title(f'è®­ç»ƒé›† {feature} åˆ†å¸ƒ')
    
    plt.tight_layout()
    plt.savefig(os.path.join(args.output_dir, 'feature_distribution.png'), dpi=300)
    
    logger.info(f"å·²ä¿å­˜åˆ†å¸ƒå¯è§†åŒ–å›¾è¡¨åˆ°ç›®å½•: {args.output_dir}")

def print_distribution(df, name):
    """æ‰“å°åˆ†å¸ƒæŠ¥å‘Š"""
    logger.info(f"\nğŸ“Š {name} æ•°æ®åˆ†å¸ƒ:")
    
    # æ€»ä½“åˆ†å¸ƒ
    logger.info(f"æ€»æ ·æœ¬æ•°: {len(df)}")
    logger.info(f"æ­£è´Ÿç±»åˆ†å¸ƒ: \n{df['label'].value_counts()}")
    
    # èº«ä½“éƒ¨ä½åˆ†å¸ƒ
    body_part_dist = df.groupby('body_part')['label'].count()
    logger.info(f"èº«ä½“éƒ¨ä½åˆ†å¸ƒ: \n{body_part_dist}")
    
    # èº«ä½“éƒ¨ä½å’Œæ ‡ç­¾çš„è¯¦ç»†åˆ†å¸ƒ
    detailed_dist = df.groupby(['body_part', 'label']).size().unstack(fill_value=0)
    detailed_dist.columns = ['æ­£å¸¸ (0)', 'å¼‚å¸¸ (1)']
    logger.info(f"è¯¦ç»†åˆ†å¸ƒ: \n{detailed_dist}")
    
    # æ­£è´Ÿç±»æ¯”ä¾‹
    detailed_dist['æ¯”ä¾‹(å¼‚å¸¸/æ€»æ•°)'] = detailed_dist['å¼‚å¸¸ (1)'] / (detailed_dist['æ­£å¸¸ (0)'] + detailed_dist['å¼‚å¸¸ (1)'])
    logger.info(f"å„éƒ¨ä½å¼‚å¸¸æ ·æœ¬æ¯”ä¾‹: \n{detailed_dist['æ¯”ä¾‹(å¼‚å¸¸/æ€»æ•°)']}")

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‚æ•°
    args = parse_args()
    args = setup_environment(args)
    
    logger.info(f"å¼€å§‹å¤„ç†MURAæ•°æ®é›†, æ•°æ®æ ¹ç›®å½•: {args.data_root}")
    
    # è·å–å›¾åƒè½¬æ¢å‡½æ•°
    transform = get_transform(args.img_size)
    
    # å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    logger.info("è·å–è®­ç»ƒé›†ç»Ÿè®¡ä¿¡æ¯...")
    train_stats = get_patient_stats(args.train_csv, args.data_root, transform, "train")
    
    logger.info("è·å–éªŒè¯é›†ç»Ÿè®¡ä¿¡æ¯...")
    valid_stats = get_patient_stats(args.valid_csv, args.data_root, transform, "valid")
    
    # åˆå¹¶æ•°æ®é›†
    df_all = pd.concat([train_stats, valid_stats], ignore_index=True)
    if df_all.empty:
        logger.error("å¤„ç†åçš„æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å’Œè·¯å¾„")
        return
    
    logger.info(f"åˆå¹¶åçš„æ•°æ®é›†å¤§å°: {df_all.shape}")
    
    # åˆ†å±‚èšç±»åˆ†å‰²
    df_train_final, df_valid_final = stratified_cluster_split(df_all, args)
    
    # æ‰“å°åˆ†å¸ƒæŠ¥å‘Š
    print_distribution(df_train_final, "è®­ç»ƒé›†")
    print_distribution(df_valid_final, "éªŒè¯é›†")
    
    # ä¿å­˜ç»“æœ
    save_results(df_train_final, df_valid_final, args)
    
    # å¯è§†åŒ–åˆ†å¸ƒ
    visualize_distribution(df_train_final, df_valid_final, args)
    
    logger.info("âœ… å®Œæˆ! æ–°çš„è®­ç»ƒ/éªŒè¯é›†CSVå·²ä¿å­˜.")

if __name__ == "__main__":
    # å‘½ä»¤è¡Œç¤ºä¾‹
    # python mura_balanced_split.py \
    #     --data_root "C:\Users\Vivo\2025_medicalimage_and_AI" \
    #     --train_csv "C:\Users\Vivo\2025_medicalimage_and_AI\MURA-v1.1\train_labeled_studies.csv" \
    #     --valid_csv "C:\Users\Vivo\2025_medicalimage_and_AI\MURA-v1.1\valid_labeled_studies.csv" \
    #     --output_dir "mura_balanced_split" \
    #     --valid_ratio 0.2 \
    #     --patient_level
    print("å‘½ä»¤è¡Œç¤ºä¾‹ï¼š")
    main()
    
    #python secversion_split.py --data_root "C:\Users\Vivo\2025_medicalimage_and_AI" --train_csv "C:\Users\Vivo\2025_medicalimage_and_AI\MURA-v1.1\train_labeled_studies.csv" --valid_csv "C:\Users\Vivo\2025_medicalimage_and_AI\MURA-v1.1\valid_labeled_studies.csv" --output_dir "C:\Users\Vivo\2025_medicalimage_and_AI\mura_balanced_split" --valid_ratio 0.2 --patient_level